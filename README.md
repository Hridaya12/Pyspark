
Big Data Research with Aggregation Pipeline, Hadoop, and PySpark
Overview
This repository contains code and documentation for conducting big data research using aggregation pipelines, Hadoop, and PySpark. The aim of this research is to analyze large datasets efficiently and derive meaningful insights using advanced data processing techniques.

Requirements
MongoDB (for aggregation pipelines)
Hadoop cluster (for distributed data processing)
Apache Spark (for PySpark)****


Follow this link for the research paper:
https://www.researchgate.net/publication/375684013_Data-Driven_Insights_and_Multidisciplinary_Strategies_for_Enhancing_Urban_Traffic_Safety_and_Mitigating_Fatalities_A_Comprehensive_Study_in_the_Context_of_a_Prominent_Metropolitan_Area
